{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"z3sg0gQ86sM9"},"source":["# Lab 5 Text Classification"]},{"cell_type":"markdown","metadata":{"id":"G_QONedFC-iz"},"source":["<h1 align=center><font size = 5> TEXT CLASSIFICATION APPLICATION: SENTIMENT ANALYSIS ON  MOVIE REVIEW</font></h1>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ePryX98wPyDQ"},"source":["In this lab session, we will use machine learning based approach to predict the polarity of a movie review. Sentiment Analysis (SA) is a text classification problem if we have a corpus sentiment (a collection of text labelled with polarity sentiment).<br>\n","\n","In figure below, part (a) a text classification model is built by training the labelled review. Part (b) is when we can predict the sentiment of a new review using the built text classification ![text classification in general](https://www.nltk.org/images/supervised-classification.png) <br>\n","\n","This tutorial uses:<br>\n","\n","1. A modifed code from [kavita Ganesan](https://kavita-ganesan.com/news-classifier-with-logistic-regression-in-python/#.XbeO9egzY2w)<br>\n","2. Dataset of  movie reviews from IMDB dataset\n","3. Machine learning package from  [scikit-learn](https://scikit-learn.org/stable/) ref:[Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf).<br>\n","**Credit to Dr.Sabrina"]},{"cell_type":"markdown","metadata":{"id":"SAQeU2hR862L"},"source":["In this lab tutorial, we aim to classify movie reviews into their respective sentiment polarity  i.e . positive or negative. Basically, the tasks include:\n","\n","- Read in a collection of documents - a corpus\n","\n","-  Transform text into numerical vector data \n","\n","- Create a classifier\n","\n","-  Fit/train the classifier\n","\n","- Test the classifier on new data\n","\n","- Evaluate performance"]},{"cell_type":"markdown","metadata":{"id":"spZJ3eQiC-i5"},"source":["###Import Libraries\n","\n","Let's start by importing the libraries.."]},{"cell_type":"code","metadata":{"id":"Jzk84k7hI3Zw"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8Y0KA1d9_eF"},"source":["## Load the dataset\n","\n","Since we are using Google Colab, you can use the following code to upload the dataset to Colab. \n","\n","Download it from UKMFolio first to your computer and choose the file when prompted."]},{"cell_type":"code","metadata":{"id":"1pKLs3Otl09U","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"343a9243-b23a-4149-c905-c9d8175d2b6a","executionInfo":{"status":"ok","timestamp":1669963165576,"user_tz":-480,"elapsed":1394863,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"source":["from google.colab import files \n","  \n","  \n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-42c1237d-d2cf-4bdc-9751-8975da89ca4e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-42c1237d-d2cf-4bdc-9751-8975da89ca4e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving IMDB Dataset.csv to IMDB Dataset.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"M54w97-2-Wl9"},"source":["Next, we can use dataframe in pandas to view the content of the csv file."]},{"cell_type":"code","metadata":{"id":"cVU88V45C-jA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669963166219,"user_tz":-480,"elapsed":652,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"18b72b95-4d9a-461f-9588-ddceb4f6c039"},"source":["# movie review - provided by IMDB\n","df= pd.read_csv(\"IMDB Dataset.csv\")\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"e31NTW7H_MvW"},"source":["Take a quick look at the 'sentiment' column"]},{"cell_type":"code","metadata":{"id":"fhHLJXz__TUE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669963166220,"user_tz":-480,"elapsed":8,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"612602e8-4d7d-4753-9566-3be1ac3a0fb3"},"source":["df['sentiment'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["positive    25000\n","negative    25000\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"lfa1P4jUQnlo"},"source":["## Feature extractor"]},{"cell_type":"markdown","metadata":{"id":"j2vnDmvl_uld"},"source":["We will be using the term-frequency (countVectorizer) and TF-IDF feature weighting. "]},{"cell_type":"code","metadata":{"id":"4pa8gDe8HP4t"},"source":["#feature extraction\n","#field = TEXT column\n","def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n","  #logging.info(\"Extracting features and creating vocabulary...\")\n","  if \"binary\" in type:\n","    cv= CountVectorizer(binary=True, max_df=0.95)\n","    cv.fit_transform(training_data[field].values)\n","    train_feature_set=cv.transform(training_data[field].values)\n","    test_feature_set=cv.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,cv\n","    #count-based representation\n","  elif \"counts\" in type:\n","    cv= CountVectorizer(binary=False, max_df=0.95)\n","    cv.fit_transform(training_data[field].values)\n","    train_feature_set=cv.transform(training_data[field].values)\n","    test_feature_set=cv.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,cv\n","  else:   \n","    # TF-IDF BASED FEATURE REPRESENTATION\n","    tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n","    tfidf_vectorizer.fit_transform(training_data[field].values)\n","    train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n","    test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,tfidf_vectorizer\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_zdJxFsFbzJ"},"source":["## Split the data into train & test sets:\n","For a supervised learning approach, we would need to split the dataset into training and test set. The training set will be used to learn the patterns from the data. \n"," The test set will be used to evaluate the performance of the classification model."]},{"cell_type":"code","metadata":{"id":"N7e5JrCuJczy"},"source":["#create features\n","#field  - column name contains the review text\n","#feature_rep   - can be binary, counts or tf\n","field = 'review'\n","feature_rep = 'tf'\n","# GET A TRAIN TEST SPLIT (set seed for consistent results)\n","training_data,testing_data = train_test_split(df,random_state = 2000)\n","# GET FEATURES\n","X_train,X_test, feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n","# GET LABELS\n","Y_train=training_data['sentiment'].values\n","Y_test=testing_data['sentiment'].values\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NxBaPttQs4N"},"source":["### Build and Evaluate Classifier model\n","In this example, two classifiers are used i.e Logistic Regression and Multinomial Naive Bayes"]},{"cell_type":"code","metadata":{"id":"DUUB3XxXKGaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669965923015,"user_tz":-480,"elapsed":4580,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"955d224b-029e-48fa-c96a-d980cf294f55"},"source":["#build the classifier model - logistic regression\n","from sklearn.linear_model import LogisticRegression\n","scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n","model_LR=scikit_log_reg.fit(X_train,Y_train)\n","lr_predicted= model_LR.predict(X_test)\n","print(\"Logistic Regression with TFIDF:\",metrics.accuracy_score(Y_test, lr_predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LibLinear]Logistic Regression with TFIDF: 0.89976\n"]}]},{"cell_type":"code","metadata":{"id":"ByufcdDnRl-L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669965923017,"user_tz":-480,"elapsed":9,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"3a2203da-fbe4-495a-c136-e995ca3e0289"},"source":["#build the classifier model - naives bayes\n","from sklearn.naive_bayes import MultinomialNB\n","model_nb = MultinomialNB().fit(X_train, Y_train)\n","nb_predicted= model_nb.predict(X_test)\n","print(\"MultinomialNB Accuracy with TFIDF:\",metrics.accuracy_score(Y_test, nb_predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MultinomialNB Accuracy with TFIDF: 0.85696\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jb8svRqlGCro"},"source":["Which classifier performs better?\n","Logistic Regression classifier has higher accuracy compared to MultinomialNB"]},{"cell_type":"markdown","metadata":{"id":"-sCKulEoGSPI"},"source":["## Lab Task\n","\n","Modify the program in  the following ways and report the trend that you observe from the output. \n","\n","- Use a different corpus with similar label. You may get the corpus from Kaggle.com or any other sources (even with different domain)\n","\n","\n","- Select the features to be used (text representation eg. TF-IDF)\n","\n","- Use three different classifiers (eg. SVM, Naive Bayes, KNN, LR etc) and observe how this affects the performance\n","\n","Write a short report on the trends that you observed. Make sure to include some comments to the changes that you made in the program.\n","\n","The deadline for this assignment is **9 Dec 2022**. Share the file link in UKMFolio."]},{"cell_type":"markdown","source":["## Selective Stock Headlines Sentiment corpus\n","Source: https://www.kaggle.com/datasets/ryanchan911/selective-stock-headlines-sentiment\n","This data-set includes social media headlines (twitter) of selective stocks and their sentiments (positive/negative). All data is from the internet and collected by Beautiful soup with basic data processing."],"metadata":{"id":"Iz6sz27-wr6i"}},{"cell_type":"markdown","source":["### Step 1: Import Libraries"],"metadata":{"id":"zS-Nj31hqXwH"}},{"cell_type":"code","metadata":{"id":"dhrrpi1-_FBU"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TB4YEqn_FBV"},"source":["###Step 2: Load the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"e8778003-7a41-4812-df8c-0930d90ab5b0","executionInfo":{"status":"ok","timestamp":1670219030511,"user_tz":-480,"elapsed":28606,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"id":"_5rCTkoA_FBV"},"source":["from google.colab import files \n","\n","#Upload Stock.csv\n","uploaded = files.upload() "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-541fa910-a48d-4355-b5e8-8fd25fefaa1a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-541fa910-a48d-4355-b5e8-8fd25fefaa1a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Stock.csv to Stock.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"adIJzZof_FBW"},"source":["Next, we can use dataframe in pandas to view the content of the csv file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670219030512,"user_tz":-480,"elapsed":27,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"74dffb1e-3a59-4fd1-feb6-7c3181764274","id":"xdJi671q_FBW"},"source":["df= pd.read_csv(\"Stock.csv\")\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                 datetime                                           headline  \\\n","0     01/16/2020 05:25 AM  $MMM fell on hard times but could be set to re...   \n","1           01-11-20 6:43  Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf...   \n","2           01-09-20 9:37  3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...   \n","3          01-08-20 17:01  $MMM #insideday follow up as it also opened up...   \n","4           01-08-20 7:44  $MMM is best #dividend #stock out there and do...   \n","...                   ...                                                ...   \n","9465        04-11-19 1:24  $WMT - Walmart shifts to remodeling vs. new st...   \n","9466        04-10-19 6:05  Walmart INC $WMT Holder Texas Permanent School...   \n","9467        04-09-19 4:38  $WMT $GILD:3 Dividend Stocks Perfect for Retir...   \n","9468        04-09-19 4:30  Walmart expanding use of #robots to scan shelv...   \n","9469        04-09-19 4:11  $WMT Walmart plans to add thousands of robot h...   \n","\n","     ticker  sentiment  \n","0       MMM          0  \n","1       MMM          1  \n","2       MMM          1  \n","3       MMM          1  \n","4       MMM          0  \n","...     ...        ...  \n","9465    WMT          1  \n","9466    WMT          0  \n","9467    WMT          1  \n","9468    WMT          1  \n","9469    WMT          1  \n","\n","[9470 rows x 4 columns]\n"]}]},{"cell_type":"markdown","source":["###Step 3: Drop datetime and ticker column\n","We will only need the headlines and sentiment in this program."],"metadata":{"id":"w6PEPR4wqhY3"}},{"cell_type":"code","source":["df = df.drop('datetime', axis=1)\n","df = df.drop('ticker', axis=1)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"FJrtdOAFn0od","executionInfo":{"status":"ok","timestamp":1670219030513,"user_tz":-480,"elapsed":25,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"fe56af5c-b245-4b37-a0a4-0a449ae508bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               headline  sentiment\n","0     $MMM fell on hard times but could be set to re...          0\n","1     Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf...          1\n","2     3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...          1\n","3     $MMM #insideday follow up as it also opened up...          1\n","4     $MMM is best #dividend #stock out there and do...          0\n","...                                                 ...        ...\n","9465  $WMT - Walmart shifts to remodeling vs. new st...          1\n","9466  Walmart INC $WMT Holder Texas Permanent School...          0\n","9467  $WMT $GILD:3 Dividend Stocks Perfect for Retir...          1\n","9468  Walmart expanding use of #robots to scan shelv...          1\n","9469  $WMT Walmart plans to add thousands of robot h...          1\n","\n","[9470 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-35d5de42-5062-469b-a67d-79baa821f376\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>headline</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>$MMM fell on hard times but could be set to re...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>$MMM #insideday follow up as it also opened up...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>$MMM is best #dividend #stock out there and do...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9465</th>\n","      <td>$WMT - Walmart shifts to remodeling vs. new st...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9466</th>\n","      <td>Walmart INC $WMT Holder Texas Permanent School...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9467</th>\n","      <td>$WMT $GILD:3 Dividend Stocks Perfect for Retir...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9468</th>\n","      <td>Walmart expanding use of #robots to scan shelv...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9469</th>\n","      <td>$WMT Walmart plans to add thousands of robot h...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9470 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35d5de42-5062-469b-a67d-79baa821f376')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35d5de42-5062-469b-a67d-79baa821f376 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35d5de42-5062-469b-a67d-79baa821f376');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["###Step 4: Replace sentiment (1 to positive) and (0 to negative)\n","So that we can differentiate the sentiment easily"],"metadata":{"id":"_O-efEt2rBJq"}},{"cell_type":"code","source":["df['sentiment'] = df['sentiment'].replace([1], 'positive')\n","df['sentiment'] = df['sentiment'].replace([0], 'negative')\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"3vjazBr8rhK_","executionInfo":{"status":"ok","timestamp":1670219030514,"user_tz":-480,"elapsed":23,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"bc10363c-401d-454f-dc3f-706093ceb74d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               headline sentiment\n","0     $MMM fell on hard times but could be set to re...  negative\n","1     Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf...  positive\n","2     3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...  positive\n","3     $MMM #insideday follow up as it also opened up...  positive\n","4     $MMM is best #dividend #stock out there and do...  negative\n","...                                                 ...       ...\n","9465  $WMT - Walmart shifts to remodeling vs. new st...  positive\n","9466  Walmart INC $WMT Holder Texas Permanent School...  negative\n","9467  $WMT $GILD:3 Dividend Stocks Perfect for Retir...  positive\n","9468  Walmart expanding use of #robots to scan shelv...  positive\n","9469  $WMT Walmart plans to add thousands of robot h...  positive\n","\n","[9470 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-8c76330c-0396-4468-9490-33ee07521703\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>headline</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>$MMM fell on hard times but could be set to re...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>$MMM #insideday follow up as it also opened up...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>$MMM is best #dividend #stock out there and do...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9465</th>\n","      <td>$WMT - Walmart shifts to remodeling vs. new st...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9466</th>\n","      <td>Walmart INC $WMT Holder Texas Permanent School...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9467</th>\n","      <td>$WMT $GILD:3 Dividend Stocks Perfect for Retir...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9468</th>\n","      <td>Walmart expanding use of #robots to scan shelv...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9469</th>\n","      <td>$WMT Walmart plans to add thousands of robot h...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9470 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c76330c-0396-4468-9490-33ee07521703')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8c76330c-0396-4468-9490-33ee07521703 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8c76330c-0396-4468-9490-33ee07521703');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"dZTlUW2Y_FBZ"},"source":["Count the sentiment which are positive and negative"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670219030519,"user_tz":-480,"elapsed":25,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"1695d41e-a58b-4559-87fb-1155d7648f1a","id":"HWpiUXXi_FBZ"},"source":["df['sentiment'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["positive    5482\n","negative    3988\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"95NH7_jz_FBZ"},"source":["###Step 5: Feature extractor\n","The features to be used is term-frequency (countVectorizer) and TF-IDF feature weighting. The main difference between the 2 implementations is that TfidfVectorizer(TF-IDF) performs both term frequency and inverse document frequency, while using TfidfTransformer will require CountVectorizer class to perform Term Frequency.\n"]},{"cell_type":"code","metadata":{"id":"-iYKgtlz_FBa"},"source":["#feature extraction\n","#field = TEXT column\n","def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n","  #logging.info(\"Extracting features and creating vocabulary...\")\n","  if \"binary\" in type:\n","    cv= CountVectorizer(binary=True, max_df=0.95)\n","    cv.fit_transform(training_data[field].values)\n","    train_feature_set=cv.transform(training_data[field].values)\n","    test_feature_set=cv.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,cv\n","    #count-based representation\n","  elif \"counts\" in type:\n","    cv= CountVectorizer(binary=False, max_df=0.95)\n","    cv.fit_transform(training_data[field].values)\n","    train_feature_set=cv.transform(training_data[field].values)\n","    test_feature_set=cv.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,cv\n","  else:   \n","    # TF-IDF BASED FEATURE REPRESENTATION\n","    tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n","    tfidf_vectorizer.fit_transform(training_data[field].values)\n","    train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n","    test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n","    return train_feature_set,test_feature_set,tfidf_vectorizer\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZ2XXEt5_FBa"},"source":["###Step 6: Split the data into train & test sets:\n","For a supervised learning approach, we would need to split the dataset into training and test set. The training set will be used to learn the patterns from the data. \n"," The test set will be used to evaluate the performance of the classification model."]},{"cell_type":"code","metadata":{"id":"RG69qpqE_FBa"},"source":["#create features\n","#field  - column name contains the review text\n","#feature_rep   - can be binary, counts or tf\n","field = 'headline'\n","feature_rep = 'tf' #we will use TF-IDF feature weighting\n","# GET A TRAIN TEST SPLIT (set seed for consistent results)\n","training_data,testing_data = train_test_split(df,random_state = 2000)\n","# GET FEATURES\n","X_train,X_test, feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n","# GET LABELS\n","Y_train=training_data['sentiment'].values\n","Y_test=testing_data['sentiment'].values\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvIEciAK_FBb"},"source":["### Step 7: Build and Evaluate Classifier model\n","Three different classifiers will be used"]},{"cell_type":"markdown","source":["**1. Logistic Regression**"],"metadata":{"id":"-18v4X0YzCaF"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670219753674,"user_tz":-480,"elapsed":1126,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"fe229162-7e8d-4cfe-8824-8b7e5f7781be","id":"R6xmgBHQ_FBb"},"source":["#build the classifier model - logistic regression\n","from sklearn.linear_model import LogisticRegression\n","scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n","model_LR=scikit_log_reg.fit(X_train,Y_train)\n","lr_predicted= model_LR.predict(X_test)\n","print(\"Logistic Regression with TFIDF:\",metrics.accuracy_score(Y_test, lr_predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LibLinear]Logistic Regression with TFIDF: 0.981418918918919\n"]}]},{"cell_type":"markdown","source":["**2. Multinomial Naive Bayes**"],"metadata":{"id":"l55ffiB6zKXA"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670219763271,"user_tz":-480,"elapsed":15,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"52a4a245-fcda-4db3-975a-a9116f6ab52e","id":"iJXCC-ia_FBc"},"source":["#build the classifier model - naives bayes\n","from sklearn.naive_bayes import MultinomialNB\n","model_nb = MultinomialNB().fit(X_train, Y_train)\n","nb_predicted= model_nb.predict(X_test)\n","print(\"MultinomialNB Accuracy with TFIDF:\",metrics.accuracy_score(Y_test, nb_predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MultinomialNB Accuracy with TFIDF: 0.8982263513513513\n"]}]},{"cell_type":"markdown","source":["**3. Support Vector Machine**"],"metadata":{"id":"RooB5tSmzPGp"}},{"cell_type":"code","source":["#Import svm model\n","from sklearn import svm\n","#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","#Create a svm Classifier\n","clf = svm.SVC(kernel='linear') # Linear Kernel\n","#Train the model using the training sets\n","clf.fit(X_train, Y_train)\n","#Predict the response for test dataset\n","y_pred = clf.predict(X_test)\n","# Model Accuracy: how often is the classifier correct?\n","print(\"SVM Accuracy with TFIDF:\",metrics.accuracy_score(Y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYgtlZsatyLG","executionInfo":{"status":"ok","timestamp":1670219765715,"user_tz":-480,"elapsed":2454,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"8bacb237-f353-4ecf-ca5e-b34e09e50e92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy with TFIDF: 0.984375\n"]}]},{"cell_type":"markdown","source":["Count Vectorizer is a way to convert a given set of strings into a frequency representation. TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions. \n","\n","The classifier that performs the best is **SVM** where its accuracy (0.984) is the highest among the other two classifier when using TF-IDF based feature reprensentation. \n","The Naive Bayes algorithm relies on an assumption of conditional independence of features given a class. SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables."],"metadata":{"id":"wYQEyvV8wYqI"}},{"cell_type":"markdown","metadata":{"id":"D4GEUj7dIPg-"},"source":["NAME: CHONG WEI YI\n","\n","MATRIC NO: A180497"]}]}